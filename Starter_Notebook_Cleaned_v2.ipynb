{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a02285e6",
      "metadata": {
        "id": "a02285e6"
      },
      "source": [
        "# Starter Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcc5329",
      "metadata": {
        "id": "bdcc5329"
      },
      "source": [
        "Install and import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 113604,
          "status": "ok",
          "timestamp": 1745025900830,
          "user": {
            "displayName": "Zheng Yang",
            "userId": "04896731693603798716"
          },
          "user_tz": -480
        },
        "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
        "outputId": "8e646f1f-d0f1-4c4e-bc5c-847d80da4d6a",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:17:34.953763Z",
          "start_time": "2025-04-21T01:17:31.845255Z"
        }
      },
      "source": [
        "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
        "!pip install nvidia-ml-py3"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\anaconda\\envs\\ml\\lib\\site-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in c:\\anaconda\\envs\\ml\\lib\\site-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in c:\\anaconda\\envs\\ml\\lib\\site-packages (0.4.3)\n",
            "Requirement already satisfied: accelerate in c:\\anaconda\\envs\\ml\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: peft in c:\\anaconda\\envs\\ml\\lib\\site-packages (0.15.1)\n",
            "Requirement already satisfied: trl in c:\\anaconda\\envs\\ml\\lib\\site-packages (0.16.1)\n",
            "Requirement already satisfied: bitsandbytes in c:\\anaconda\\envs\\ml\\lib\\site-packages (0.45.5)\n",
            "Requirement already satisfied: filelock in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\anaconda\\envs\\ml\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in c:\\anaconda\\envs\\ml\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\anaconda\\envs\\ml\\lib\\site-packages (from datasets) (3.11.16)\n",
            "Requirement already satisfied: psutil in c:\\anaconda\\envs\\ml\\lib\\site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from accelerate) (2.6.0+cu118)\n",
            "Requirement already satisfied: rich in c:\\anaconda\\envs\\ml\\lib\\site-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in c:\\anaconda\\envs\\ml\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\anaconda\\envs\\ml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from rich->trl) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\envs\\ml\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: nvidia-ml-py3 in c:\\anaconda\\envs\\ml\\lib\\site-packages (7.352.0)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
      "metadata": {
        "executionInfo": {
          "elapsed": 31470,
          "status": "ok",
          "timestamp": 1745025945780,
          "user": {
            "displayName": "Zheng Yang",
            "userId": "04896731693603798716"
          },
          "user_tz": -480
        },
        "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:17:46.903495Z",
          "start_time": "2025-04-21T01:17:36.676395Z"
        }
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\anaconda\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\anaconda\\envs\\ml\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "id": "59d6e377",
      "metadata": {
        "id": "59d6e377"
      },
      "source": [
        "## Load Tokenizer and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522,
          "referenced_widgets": [
            "6b3baa1c162146b4a4188e6b920910d2",
            "7b1fbe03186d45d493a3756cd52f8fd1",
            "2d76455f50ca4fe796ece7de3bb939ae",
            "232b28ac7ade46f2b2b16fe4565a445e",
            "eb390f5def224e0c98bcd74b7c0343ff",
            "8b062041f6cd4e88839fa6199eca2fff",
            "8fc573296ccf4e5681b1670d74b18504",
            "1c6d2dee8d1a44fba756dcdd9038ac18",
            "1402fa037adf470a8e4f939a130f1491",
            "8a85a83ea7d745cea89765d3760dd5dd",
            "7485296b4bb540b9ac19cdee37db431b",
            "8d00d3deba814e20ad36d2447cba21df",
            "4bcbed33bca3402dafa12135dea92309",
            "6a4349a37bcf496f939402099d696025",
            "6973777764bb452ba76776d2c529538c",
            "87f6a9182834479681e217eb6be6d7d8",
            "19d1de80ea8b42a18502eecd86623308",
            "97bc188200c94669b6f308774141a105",
            "62615f1707984b2088c0517c65669427",
            "b8ea468f029d4b41b1c840d459474742",
            "79ff42b61abe4ca18bca726198ea6f58",
            "bafd85de435d4c069ee8e286b6a80aa6",
            "b5df3a3382c045f2a4d4887d6cf0ccf5",
            "c0fc214cb736489f9ee5797e0eaaee7e",
            "528810a2987a416d84671c2bc6acf97f",
            "8af6c99930174688aa2bd1c92d25fd15",
            "061407ef90fd400a8c6cc8c2ea23b0c0",
            "21fceda620624a6c80edbc50783bd31a",
            "eaba1974d7434d538aa8b7bd88eaeb6d",
            "0f501a2c98a84b3597ee4c7887eaae26",
            "7d53e5f0dfa24da39e09937c112fccd4",
            "1735ffbd709841c5aa3b0aa4ec452d61",
            "94f799af89fa42dbbd7dea66e4a25e99",
            "826aca5a3fc34a6ba095bef1d8542e92",
            "aff0e0cee90346bb9bd259487f71b39a",
            "ab68290c25af4e8bad58aa28b4a98b51",
            "220d765568ee43c992b823b9b68deb5b",
            "040e8f124d3e4c61a017d3a407b351a2",
            "c0a22ace76924ab8987fb4d4ae131b8f",
            "9dfd025e78bf4418baa46cdde1f4d5ec",
            "0665238efede4575ae6cf50aacc9a1a3",
            "fd47da103b0f4b20822f5ab35271a3c3",
            "f378492ec3b245d1991337496c88653a",
            "edd675781dd4461db1f2c9952708a4e8",
            "4b95990642c9465ebea1cc0fe16be97a",
            "5de3b8d1cb8347e4add04e59abcbfaff",
            "9de71a1ac0b341cf9f611fc345a12976",
            "8e63b8e3baf24aafbbc7f595abb3b1ff",
            "89a65f4f6c0d4738b34ba67b0ad377ab",
            "fc821d3bf01a42ab9f5901643f583c02",
            "c8e93109c37e48ea96f3693eb49e42f1",
            "cd393cda0f8b4ca9ac64d7f3ffa2acfa",
            "3cf500d83b52471f9e31c6ab230b189f",
            "ac3eace9b02a413498ce45d6ecac2e79",
            "1189e38905bc49459598af168156eaa7",
            "c5895790342f46ee9ad541c30a434862",
            "769f07273ab0438dba065a6d88b2d0a9",
            "7e8b713e6d9b4235bc62920fa08b6a71",
            "0a342c6ae54f47cea2ac27611a3bc967",
            "756957345a3d40c194c63b558f741902",
            "d696d16e01c046e78db6946f948866de",
            "18c26f661e004a778aead78cfd248b1a",
            "2121f4a64f994eda9356b580dd3f8e7d",
            "be5d8ce4106948c1843029e7f3a02266",
            "1bd426f4498f49ac85b57ddb46d576e7",
            "57e16f0199d2429db405f65701cad447",
            "c44f71a279a24998bf798fcffb98279c",
            "37e2e6d3f8a942a4a28ebb7228a3aa3c",
            "fe79b0b2a8a84b479f1a0ca3b9b9a583",
            "386d7fa119744e0d85a0425cc61fcd86",
            "94ed85ded87346aaa3b1cdb80cbfd867",
            "8dc177a1bb234c758e6936ded8d0b65b",
            "05f73335896240ada5529efae0a2f5be",
            "b09ec694fdff4e4292728b47eaa9c5d8",
            "a47d29dd78b443f1893d271ef57329f7",
            "44ca703981ab4d75960768beaefaa60b",
            "dc64d68de64c42a982d7f79e9f000cb2",
            "338b5f0670274e1c9c27f0dae2b99497",
            "fe9d45d4fdca4f0fa40bb33aeff1bbc1",
            "463701d23a9a41fea557ad1351b41a63",
            "a2ee89b88dd34c529fd20071f41c0200",
            "24810837bffc4ff1b281cf3d2595bae1",
            "4c7785e0552341efb79c5f69eb15417e",
            "899370a900ed43fea4331f9eabe65f9d",
            "467ef4b5654c42cab0469d58b568acc3",
            "e5b703c519ef46c894b4496611bf1974",
            "03e4dc7e8f414e3897c6f900b8ad1bbf",
            "c2fe8d7762214f0d8d6921859200cff7",
            "da5b7d768dd54304a6f10d7833489d6d",
            "712ed580ed1845e78c04361a8ce6be9a",
            "58234badc2a34cdf8ed24affbf68e377",
            "daecd0085514444cbe6b091f82feb459",
            "71d5b0ceba5c4eb288e4f62cb0adb886",
            "fb8cfbe14c304f57aa9d393364354f2c",
            "b33ec3c7581543b99c7b6418d1e5b825",
            "969deeec6900484e98fc0944a99ce63f",
            "21e86fd8afc34dee8553ff244ac135c7",
            "ca96e7f4ad9447a2b8c83c4f7cc43fd1",
            "b21363c3b3e14ddcb537691d0add33ec",
            "1c0ef1362b78423d98530ad515304f5e",
            "99f41d9f89ca4791ba43b5c17513a66d",
            "06a448da5fe244ebbd3a52757c5260c9",
            "69272c0a0858486682ff0c3b58330f9f",
            "6278a25d4dcb443e84468ef9d0bb5c18",
            "12ceba395ae14d2580f625874c1c7d37",
            "ce07f7a047a94a719b0598c8bf8e1a86",
            "c3506d32c41a4e16bb44960ed5d00ceb",
            "465a33b5be3b40d39d9ad5814479c28c",
            "e794467c742849f48c35004379eaa8c2",
            "c4f38230335844ec8b3f1b06a57318eb",
            "6c9454d7177f40fbb31c6642b649119e",
            "486ade53edf743dcaa79b523b582be57",
            "deb87f6815a942c4abc51cb3b1566636",
            "421876b5aac342009d3d566d7e4fd0ef",
            "ca0107fea21d43d8b35ad27d9a1f87ff",
            "08ed05910b284e46b7db5214bfa51978",
            "ed15c7130d974c12b692b860b80bac53",
            "eec4c9f2480d480c8f0ba98fca684f29",
            "91ea0f2bc316474db09a670b93c6d8ce",
            "8a00c117ce864b6bb8fb21f22be13844",
            "c6088058211b443b834ad1e27b94c37e",
            "4a4891224a8a4967aa068a122c7b7c65",
            "e7b0b9b3a1a049c4978baa4f4d1396ef",
            "a0ab00a640c242d6a558217e32b9d6e2",
            "e20432e33e7e4923ae1f9f54b6c08626",
            "e5ccf7632e8f479fa3feedf81cf49c8b",
            "dd8cfac7849c4c6b9b14cc65d9edae54",
            "dd7cd41160e1484fb3e9d754dd05ca7e",
            "70f0f51316a84785aa78dc79186a17d4",
            "450d38fa582148fca3ac786289cd1384",
            "08c3acedab46415eb1f088b4ead61d2b",
            "a10beb7e99d24606a0bb0a71583875f5"
          ]
        },
        "executionInfo": {
          "elapsed": 44383,
          "status": "ok",
          "timestamp": 1745025994002,
          "user": {
            "displayName": "Zheng Yang",
            "userId": "04896731693603798716"
          },
          "user_tz": -480
        },
        "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
        "outputId": "36a0a2c1-3054-4e18-9d47-669018d39f33",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:17:50.083354Z",
          "start_time": "2025-04-21T01:17:48.573133Z"
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "output_dir = f\"./results/{timestamp}\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# -----------------------------\n",
        "# 3. Load and preprocess AGNEWS dataset\n",
        "# -----------------------------\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "d0bd1684d5614e95affea0db203b1784",
            "0841c3e6518b4b628b92911a43c298a5",
            "1df184da0e7b4d21a24bc88e9e1651c0",
            "348db98b95f94dc1a3f037efe06a8eb1",
            "630ae54a7b6d4eb1913c826f7ad4cb96",
            "07071996e2c64b92860f56d21a0aaecc",
            "009b3c8525be4b04b053c683a91bfeed",
            "57bbbfbffefd453086328c9e9607e730",
            "151c7dbeb42b429fa74086df09438bb2",
            "84dc5ae07ab14811a71f6735f259b773",
            "ebff85d048f0463ca0bbc2038e753361"
          ]
        },
        "executionInfo": {
          "elapsed": 10220,
          "status": "ok",
          "timestamp": 1745026007303,
          "user": {
            "displayName": "Zheng Yang",
            "userId": "04896731693603798716"
          },
          "user_tz": -480
        },
        "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
        "outputId": "5ffde5eb-306e-434c-8b5c-7ad6c6ee5b41",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:17:51.171056Z",
          "start_time": "2025-04-21T01:17:50.715641Z"
        }
      },
      "source": [
        "# -----------------------------\n",
        "# 4. Load RoBERTa model with LoRA adapters\n",
        "# -----------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.to(device)\n",
        "model.print_trainable_parameters()\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "id": "c9e24afd",
      "metadata": {
        "id": "c9e24afd"
      },
      "source": [
        "## Load Pre-trained Model\n",
        "Set up config for pretrained model and download it from hugging face"
      ]
    },
    {
      "cell_type": "code",
      "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
      "metadata": {
        "executionInfo": {
          "elapsed": 22,
          "status": "ok",
          "timestamp": 1745026011445,
          "user": {
            "displayName": "Zheng Yang",
            "userId": "04896731693603798716"
          },
          "user_tz": -480
        },
        "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:17:52.803465Z",
          "start_time": "2025-04-21T01:17:52.756172Z"
        }
      },
      "source": [
        "# -----------------------------\n",
        "# 5. Define training arguments\n",
        "# -----------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "if os.path.exists(\"./results\"):\n",
        "    shutil.rmtree(\"./results\")\n",
        "os.makedirs(\"./results\")"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "id": "f265839d-a088-4693-8474-862641de11ed",
      "metadata": {
        "id": "f265839d-a088-4693-8474-862641de11ed"
      },
      "source": [
        "## Anything from here on can be modified"
      ]
    },
    {
      "cell_type": "code",
      "id": "e7413430-be57-482b-856e-36bd4ba799df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "e7413430-be57-482b-856e-36bd4ba799df",
        "outputId": "89bc736a-44a3-49b8-cb4d-62ee8032664c",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:57:49.201184Z",
          "start_time": "2025-04-21T01:17:54.471687Z"
        }
      },
      "source": [
        "# -----------------------------\n",
        "# 6. Train the model\n",
        "# -----------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "best_model_path = f\"{output_dir}/best_model\"\n",
        "trainer.save_model(best_model_path)\n",
        "print(f\"Best model saved to {best_model_path}\")"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_110200\\4064744788.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37500/37500 39:54, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.223100</td>\n",
              "      <td>0.194306</td>\n",
              "      <td>0.935263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.187799</td>\n",
              "      <td>0.942368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.165900</td>\n",
              "      <td>0.178437</td>\n",
              "      <td>0.945395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.179631</td>\n",
              "      <td>0.945789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.126400</td>\n",
              "      <td>0.178965</td>\n",
              "      <td>0.947105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved to ./results/2025-04-20_21-17-48/best_model\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "id": "2b73b734a3824164",
      "metadata": {
        "id": "2b73b734a3824164",
        "outputId": "8f093d4b-d638-4b9f-a614-595726179306",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:58:01.103609Z",
          "start_time": "2025-04-21T01:58:00.830532Z"
        }
      },
      "source": [
        "log_history = trainer.state.log_history\n",
        "\n",
        "if not log_history:\n",
        "    print(\"skip ploting\")\n",
        "else:\n",
        "    train_loss = []\n",
        "    eval_loss = []\n",
        "    eval_accuracy = []\n",
        "    steps = []\n",
        "    epochs = []\n",
        "\n",
        "    for log in log_history:\n",
        "        if \"loss\" in log and \"step\" in log:\n",
        "            train_loss.append(log[\"loss\"])\n",
        "            steps.append(log[\"step\"])\n",
        "        if \"eval_loss\" in log:\n",
        "            eval_loss.append(log[\"eval_loss\"])\n",
        "            eval_accuracy.append(log.get(\"eval_accuracy\", None))\n",
        "            epochs.append(log.get(\"epoch\", len(epochs) + 1))\n",
        "\n",
        "    if not train_loss or not eval_loss:\n",
        "        print(\"too less data to praint\")\n",
        "    else:\n",
        "        \n",
        "        eval_loss_extended = []\n",
        "        steps_per_epoch = len(train_loss) // len(eval_loss) if len(eval_loss) > 0 else 1\n",
        "        for i in range(len(eval_loss)):\n",
        "            eval_loss_extended.extend([eval_loss[i]] * steps_per_epoch)\n",
        "        eval_loss_extended = eval_loss_extended[:len(train_loss)]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(steps, train_loss, label=\"training loss\", color=\"blue\")\n",
        "        plt.plot(steps, eval_loss_extended, label=\"val loss\", color=\"orange\", linestyle=\"--\")\n",
        "        plt.xlabel(\"training step\")\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.title(\"taining adn val loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f\"{output_dir}/loss_curve.png\")\n",
        "        plt.close()\n",
        "        print(f\"loss plot saved to {output_dir}/loss_curve.png\")\n",
        "\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(epochs, eval_loss, label=\"val loss\", color=\"orange\", marker=\"o\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.title(\"loss in each epoch\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f\"{output_dir}/epoch_loss_curve.png\")\n",
        "        plt.close()\n",
        "        print(f\"plot saved to {output_dir}/epoch_loss_curve.png\")\n",
        "\n",
        "    if not eval_accuracy or all(acc is None for acc in eval_accuracy):\n",
        "        print(\"skip ploting\")\n",
        "    else:\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(epochs, eval_accuracy, label=\"val acc\", color=\"green\", marker=\"o\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(\"acc\")\n",
        "        plt.title(\"every acc in each epoch\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f\"{output_dir}/accuracy_curve.png\")\n",
        "        plt.close()\n",
        "        print(f\"plot saved to {output_dir}/accuracy_curve.png\")\n",
        "\n",
        "\n",
        "with open(f\"{output_dir}/training_log.json\", \"w\") as f:\n",
        "    json.dump(log_history, f)\n",
        "print(f\"training saved to {output_dir}/training_log.json\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss plot saved to ./results/2025-04-20_21-17-48/loss_curve.png\n",
            "plot saved to ./results/2025-04-20_21-17-48/epoch_loss_curve.png\n",
            "plot saved to ./results/2025-04-20_21-17-48/accuracy_curve.png\n",
            "training saved to ./results/2025-04-20_21-17-48/training_log.json\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "id": "652452e3",
      "metadata": {
        "id": "652452e3"
      },
      "source": [
        "## Setup LoRA Config\n",
        "Setup PEFT config and get peft model for finetuning"
      ]
    },
    {
      "cell_type": "code",
      "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
      "metadata": {
        "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
        "outputId": "9ce8f0a2-b794-4ef2-8b3c-65d0980c91c7",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:58:19.557216Z",
          "start_time": "2025-04-21T01:58:07.503691Z"
        }
      },
      "source": [
        "# -----------------------------\n",
        "# 7. Evaluate the model\n",
        "# -----------------------------\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Final Evaluation Accuracy:\", eval_results[\"eval_accuracy\"])"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [119/119 00:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Evaluation Accuracy: 0.9453947368421053\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0",
      "metadata": {
        "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0",
        "outputId": "da7dd063-12e2-45d4-b477-181ffb732ea2",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:58:32.366923Z",
          "start_time": "2025-04-21T01:58:32.361922Z"
        }
      },
      "source": [
        "# -----------------------------\n",
        "# 8. Check trainable parameter count\n",
        "# -----------------------------\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 888580\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
      "metadata": {
        "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
        "outputId": "0ac1c892-10f6-4687-808c-f2d67062d881",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:58:34.712685Z",
          "start_time": "2025-04-21T01:58:34.708578Z"
        }
      },
      "source": [
        "# print(\"Trainable parameters:\")\n",
        "# for name, param in peft_model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name)\n",
        "with open(f\"{output_dir}/eval_results.json\", \"w\") as f:\n",
        "    json.dump(eval_results, f)\n",
        "print(f\"accessment saved to {output_dir}/eval_results.json\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accessment saved to ./results/2025-04-20_21-17-48/eval_results.json\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
      "metadata": {
        "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
        "outputId": "9b40b903-97da-4ea2-dc5f-6d7f5a160377",
        "ExecuteTime": {
          "end_time": "2025-04-21T01:58:49.883233Z",
          "start_time": "2025-04-21T01:58:36.752887Z"
        }
      },
      "source": [
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "if not os.path.exists(best_model_path):\n",
        "    print(f\"no best model found in {best_model_path}\")\n",
        "else:\n",
        "    # \u52a0\u8f7d\u6700\u4f73\u6a21\u578b\n",
        "    print(f\"load best model from {best_model_path}\")\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
        "    best_model = PeftModel.from_pretrained(base_model, best_model_path)\n",
        "    best_model.to(device)\n",
        "\n",
        "    \n",
        "    test_data_path = \"./test_unlabelled.pkl\"\n",
        "    print(f\"check testing data: {test_data_path}\")\n",
        "    print(f\"wether exist: {os.path.exists(test_data_path)}\")\n",
        "    if not os.path.exists(test_data_path):\n",
        "        print(f\"no tset data in {test_data_path}\")\n",
        "        parent_dir = os.path.dirname(test_data_path)\n",
        "        if os.path.exists(parent_dir):\n",
        "            print(f\"dict: {os.listdir(parent_dir)}\")\n",
        "        else:\n",
        "            print(f\" {parent_dir}not exist\")\n",
        "    else:\n",
        "        try:\n",
        "            with open(test_data_path, \"rb\") as f:\n",
        "                test_dataset = pickle.load(f)\n",
        "            print(f\"sucessfully load test data, number of samples: {len(test_dataset['text'])}\")\n",
        "\n",
        "            test_dataset = Dataset.from_dict({\"text\": test_dataset[\"text\"]})\n",
        "\n",
        "            def preprocess_function(examples):\n",
        "                return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "            tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "            tokenized_test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "            test_dataloader = DataLoader(tokenized_test_dataset, batch_size=64)\n",
        "\n",
        "            \n",
        "            best_model.eval()\n",
        "            all_predictions = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in test_dataloader:\n",
        "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                    outputs = best_model(**batch)\n",
        "                    preds = torch.argmax(outputs.logits, dim=-1)\n",
        "                    all_predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "            if all_predictions:\n",
        "                df = pd.DataFrame({\n",
        "                    \"ID\": list(range(len(all_predictions))),\n",
        "                    \"label\": all_predictions\n",
        "                })\n",
        "                df.to_csv(f\"{output_dir}/submission.csv\", index=False)\n",
        "                print(f\"\u2705 prediction saved to{output_dir}/submission.csv\")\n",
        "            else:\n",
        "                print(\"no prediction\")\n",
        "        except Exception as e:\n",
        "            print(f\"error: {str(e)}\")\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load best model from ./results/2025-04-20_21-17-48/best_model\n",
            "check testing data: ./test_unlabelled.pkl\n",
            "wether exist: True\n",
            "sucessfully load test data, number of samples: 8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8000/8000 [00:00<00:00, 21145.51 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 prediction saved to./results/2025-04-20_21-17-48/submission.csv\n"
          ]
        }
      ],
      "execution_count": 11
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}