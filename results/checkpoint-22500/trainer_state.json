{
  "best_global_step": 22500,
  "best_metric": 0.17843714356422424,
  "best_model_checkpoint": "./results\\checkpoint-22500",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 22500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 2.5077884197235107,
      "learning_rate": 0.00019946666666666667,
      "loss": 0.7829,
      "step": 100
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 1.3159306049346924,
      "learning_rate": 0.00019893333333333336,
      "loss": 0.3737,
      "step": 200
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.941394090652466,
      "learning_rate": 0.0001984,
      "loss": 0.3395,
      "step": 300
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 6.9194722175598145,
      "learning_rate": 0.00019786666666666666,
      "loss": 0.3125,
      "step": 400
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.426767349243164,
      "learning_rate": 0.00019733333333333335,
      "loss": 0.3056,
      "step": 500
    },
    {
      "epoch": 0.08,
      "grad_norm": 4.03044319152832,
      "learning_rate": 0.0001968,
      "loss": 0.3025,
      "step": 600
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 1.8387162685394287,
      "learning_rate": 0.00019626666666666668,
      "loss": 0.2832,
      "step": 700
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 2.1251935958862305,
      "learning_rate": 0.00019573333333333334,
      "loss": 0.247,
      "step": 800
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.409635543823242,
      "learning_rate": 0.0001952,
      "loss": 0.2908,
      "step": 900
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.2584872245788574,
      "learning_rate": 0.0001946666666666667,
      "loss": 0.2352,
      "step": 1000
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 3.7910687923431396,
      "learning_rate": 0.00019413333333333335,
      "loss": 0.2606,
      "step": 1100
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.443060874938965,
      "learning_rate": 0.00019360000000000002,
      "loss": 0.2597,
      "step": 1200
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.5337929725646973,
      "learning_rate": 0.00019306666666666668,
      "loss": 0.2407,
      "step": 1300
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 3.844996690750122,
      "learning_rate": 0.00019253333333333334,
      "loss": 0.2506,
      "step": 1400
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.753662109375,
      "learning_rate": 0.000192,
      "loss": 0.2311,
      "step": 1500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 3.0545969009399414,
      "learning_rate": 0.0001914666666666667,
      "loss": 0.2626,
      "step": 1600
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 3.1066291332244873,
      "learning_rate": 0.00019093333333333333,
      "loss": 0.2349,
      "step": 1700
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.379125952720642,
      "learning_rate": 0.0001904,
      "loss": 0.2487,
      "step": 1800
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 4.083192825317383,
      "learning_rate": 0.00018986666666666668,
      "loss": 0.2292,
      "step": 1900
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 4.239911079406738,
      "learning_rate": 0.00018933333333333335,
      "loss": 0.2224,
      "step": 2000
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3911685049533844,
      "learning_rate": 0.0001888,
      "loss": 0.2408,
      "step": 2100
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 4.376596450805664,
      "learning_rate": 0.00018826666666666667,
      "loss": 0.2621,
      "step": 2200
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 2.3605659008026123,
      "learning_rate": 0.00018773333333333333,
      "loss": 0.2561,
      "step": 2300
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.494533061981201,
      "learning_rate": 0.00018720000000000002,
      "loss": 0.2112,
      "step": 2400
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5646648406982422,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.2359,
      "step": 2500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 2.68953275680542,
      "learning_rate": 0.00018613333333333335,
      "loss": 0.2564,
      "step": 2600
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.862245559692383,
      "learning_rate": 0.0001856,
      "loss": 0.2322,
      "step": 2700
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 2.9685778617858887,
      "learning_rate": 0.00018506666666666667,
      "loss": 0.2418,
      "step": 2800
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 2.293229579925537,
      "learning_rate": 0.00018453333333333334,
      "loss": 0.2674,
      "step": 2900
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.820371627807617,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.2679,
      "step": 3000
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 2.4491689205169678,
      "learning_rate": 0.00018346666666666666,
      "loss": 0.2046,
      "step": 3100
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 4.457486629486084,
      "learning_rate": 0.00018293333333333333,
      "loss": 0.1891,
      "step": 3200
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.5425662994384766,
      "learning_rate": 0.00018240000000000002,
      "loss": 0.1905,
      "step": 3300
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 2.5979502201080322,
      "learning_rate": 0.00018186666666666668,
      "loss": 0.2234,
      "step": 3400
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 4.1576313972473145,
      "learning_rate": 0.00018133333333333334,
      "loss": 0.276,
      "step": 3500
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.1231117248535156,
      "learning_rate": 0.0001808,
      "loss": 0.225,
      "step": 3600
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 3.5452146530151367,
      "learning_rate": 0.00018026666666666667,
      "loss": 0.217,
      "step": 3700
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 3.6710588932037354,
      "learning_rate": 0.00017973333333333333,
      "loss": 0.2192,
      "step": 3800
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.9784562587738037,
      "learning_rate": 0.00017920000000000002,
      "loss": 0.2168,
      "step": 3900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 10.686019897460938,
      "learning_rate": 0.00017866666666666668,
      "loss": 0.2177,
      "step": 4000
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 3.1793529987335205,
      "learning_rate": 0.00017813333333333334,
      "loss": 0.2141,
      "step": 4100
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.30242121219635,
      "learning_rate": 0.0001776,
      "loss": 0.2214,
      "step": 4200
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 4.528294086456299,
      "learning_rate": 0.00017706666666666667,
      "loss": 0.2292,
      "step": 4300
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 5.932546615600586,
      "learning_rate": 0.00017653333333333336,
      "loss": 0.2374,
      "step": 4400
    },
    {
      "epoch": 0.6,
      "grad_norm": 4.826114654541016,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.1868,
      "step": 4500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 2.4252843856811523,
      "learning_rate": 0.00017546666666666666,
      "loss": 0.2062,
      "step": 4600
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.6499724984169006,
      "learning_rate": 0.00017493333333333335,
      "loss": 0.1968,
      "step": 4700
    },
    {
      "epoch": 0.64,
      "grad_norm": 6.108627796173096,
      "learning_rate": 0.0001744,
      "loss": 0.238,
      "step": 4800
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 2.708226203918457,
      "learning_rate": 0.00017386666666666667,
      "loss": 0.1847,
      "step": 4900
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.524578094482422,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.2039,
      "step": 5000
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.907204508781433,
      "learning_rate": 0.0001728,
      "loss": 0.1954,
      "step": 5100
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.19300860166549683,
      "learning_rate": 0.00017226666666666666,
      "loss": 0.1889,
      "step": 5200
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 2.545121192932129,
      "learning_rate": 0.00017173333333333335,
      "loss": 0.2115,
      "step": 5300
    },
    {
      "epoch": 0.72,
      "grad_norm": 5.921610355377197,
      "learning_rate": 0.00017120000000000001,
      "loss": 0.208,
      "step": 5400
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.8689266443252563,
      "learning_rate": 0.00017066666666666668,
      "loss": 0.2309,
      "step": 5500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.4140701293945312,
      "learning_rate": 0.00017013333333333334,
      "loss": 0.2129,
      "step": 5600
    },
    {
      "epoch": 0.76,
      "grad_norm": 3.4782423973083496,
      "learning_rate": 0.0001696,
      "loss": 0.2147,
      "step": 5700
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.6233409643173218,
      "learning_rate": 0.0001690666666666667,
      "loss": 0.1861,
      "step": 5800
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 4.2014617919921875,
      "learning_rate": 0.00016853333333333336,
      "loss": 0.1996,
      "step": 5900
    },
    {
      "epoch": 0.8,
      "grad_norm": 3.895935297012329,
      "learning_rate": 0.000168,
      "loss": 0.2145,
      "step": 6000
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.43842846155166626,
      "learning_rate": 0.00016746666666666668,
      "loss": 0.2148,
      "step": 6100
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1.830615758895874,
      "learning_rate": 0.00016693333333333334,
      "loss": 0.1947,
      "step": 6200
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.7410454750061035,
      "learning_rate": 0.0001664,
      "loss": 0.2002,
      "step": 6300
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 5.731260299682617,
      "learning_rate": 0.00016586666666666667,
      "loss": 0.2186,
      "step": 6400
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 3.195631265640259,
      "learning_rate": 0.00016533333333333333,
      "loss": 0.234,
      "step": 6500
    },
    {
      "epoch": 0.88,
      "grad_norm": 5.63214111328125,
      "learning_rate": 0.0001648,
      "loss": 0.2105,
      "step": 6600
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 9.188464164733887,
      "learning_rate": 0.00016426666666666668,
      "loss": 0.1969,
      "step": 6700
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.7610036134719849,
      "learning_rate": 0.00016373333333333335,
      "loss": 0.2092,
      "step": 6800
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.7952152490615845,
      "learning_rate": 0.0001632,
      "loss": 0.2047,
      "step": 6900
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 2.8506295680999756,
      "learning_rate": 0.00016266666666666667,
      "loss": 0.1736,
      "step": 7000
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.24318522214889526,
      "learning_rate": 0.00016213333333333334,
      "loss": 0.2114,
      "step": 7100
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.6609253883361816,
      "learning_rate": 0.00016160000000000002,
      "loss": 0.2015,
      "step": 7200
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 1.8246269226074219,
      "learning_rate": 0.0001610666666666667,
      "loss": 0.233,
      "step": 7300
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 4.406670570373535,
      "learning_rate": 0.00016053333333333332,
      "loss": 0.2262,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.7342885732650757,
      "learning_rate": 0.00016,
      "loss": 0.2231,
      "step": 7500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9352631578947368,
      "eval_loss": 0.19430629909038544,
      "eval_runtime": 11.731,
      "eval_samples_per_second": 647.853,
      "eval_steps_per_second": 10.144,
      "step": 7500
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 2.2059640884399414,
      "learning_rate": 0.00015946666666666668,
      "loss": 0.1749,
      "step": 7600
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 2.2172412872314453,
      "learning_rate": 0.00015893333333333334,
      "loss": 0.1944,
      "step": 7700
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.141294479370117,
      "learning_rate": 0.00015840000000000003,
      "loss": 0.2222,
      "step": 7800
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 1.85355806350708,
      "learning_rate": 0.00015786666666666666,
      "loss": 0.1859,
      "step": 7900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.7649054527282715,
      "learning_rate": 0.00015733333333333333,
      "loss": 0.187,
      "step": 8000
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.08472251147031784,
      "learning_rate": 0.00015680000000000002,
      "loss": 0.1886,
      "step": 8100
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 2.8683223724365234,
      "learning_rate": 0.00015626666666666668,
      "loss": 0.1945,
      "step": 8200
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 1.655585527420044,
      "learning_rate": 0.00015573333333333334,
      "loss": 0.1825,
      "step": 8300
    },
    {
      "epoch": 1.12,
      "grad_norm": 4.6581292152404785,
      "learning_rate": 0.0001552,
      "loss": 0.2018,
      "step": 8400
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 4.684289932250977,
      "learning_rate": 0.00015466666666666667,
      "loss": 0.2243,
      "step": 8500
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.3367517590522766,
      "learning_rate": 0.00015413333333333336,
      "loss": 0.1785,
      "step": 8600
    },
    {
      "epoch": 1.16,
      "grad_norm": 4.39809513092041,
      "learning_rate": 0.00015360000000000002,
      "loss": 0.1749,
      "step": 8700
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 2.240100622177124,
      "learning_rate": 0.00015306666666666666,
      "loss": 0.1792,
      "step": 8800
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 3.2520017623901367,
      "learning_rate": 0.00015253333333333335,
      "loss": 0.1615,
      "step": 8900
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.883192777633667,
      "learning_rate": 0.000152,
      "loss": 0.1978,
      "step": 9000
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 2.6009719371795654,
      "learning_rate": 0.00015146666666666667,
      "loss": 0.1779,
      "step": 9100
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 3.6272575855255127,
      "learning_rate": 0.00015093333333333336,
      "loss": 0.1704,
      "step": 9200
    },
    {
      "epoch": 1.24,
      "grad_norm": 3.9018936157226562,
      "learning_rate": 0.0001504,
      "loss": 0.1766,
      "step": 9300
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 2.900808095932007,
      "learning_rate": 0.00014986666666666666,
      "loss": 0.2132,
      "step": 9400
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.1275758296251297,
      "learning_rate": 0.00014933333333333335,
      "loss": 0.2022,
      "step": 9500
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.764496922492981,
      "learning_rate": 0.0001488,
      "loss": 0.1721,
      "step": 9600
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 3.361412286758423,
      "learning_rate": 0.00014826666666666667,
      "loss": 0.2086,
      "step": 9700
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 5.407174110412598,
      "learning_rate": 0.00014773333333333334,
      "loss": 0.1812,
      "step": 9800
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.8461543321609497,
      "learning_rate": 0.0001472,
      "loss": 0.185,
      "step": 9900
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.5646642446517944,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.1756,
      "step": 10000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.14411930739879608,
      "learning_rate": 0.00014613333333333335,
      "loss": 0.2044,
      "step": 10100
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 3.499546527862549,
      "learning_rate": 0.00014560000000000002,
      "loss": 0.1752,
      "step": 10200
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 3.213646173477173,
      "learning_rate": 0.00014506666666666668,
      "loss": 0.1762,
      "step": 10300
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 2.4157934188842773,
      "learning_rate": 0.00014453333333333334,
      "loss": 0.2089,
      "step": 10400
    },
    {
      "epoch": 1.4,
      "grad_norm": 5.8971943855285645,
      "learning_rate": 0.000144,
      "loss": 0.1823,
      "step": 10500
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 4.140275478363037,
      "learning_rate": 0.0001434666666666667,
      "loss": 0.202,
      "step": 10600
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 2.0281827449798584,
      "learning_rate": 0.00014293333333333333,
      "loss": 0.1709,
      "step": 10700
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.3303791284561157,
      "learning_rate": 0.0001424,
      "loss": 0.1943,
      "step": 10800
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 2.7685537338256836,
      "learning_rate": 0.00014186666666666668,
      "loss": 0.1784,
      "step": 10900
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 1.4500885009765625,
      "learning_rate": 0.00014133333333333334,
      "loss": 0.1982,
      "step": 11000
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.7762629985809326,
      "learning_rate": 0.0001408,
      "loss": 0.166,
      "step": 11100
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 4.231761455535889,
      "learning_rate": 0.00014026666666666667,
      "loss": 0.1858,
      "step": 11200
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 5.648587226867676,
      "learning_rate": 0.00013973333333333333,
      "loss": 0.1712,
      "step": 11300
    },
    {
      "epoch": 1.52,
      "grad_norm": 4.723286151885986,
      "learning_rate": 0.0001392,
      "loss": 0.1549,
      "step": 11400
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 3.459956645965576,
      "learning_rate": 0.00013866666666666669,
      "loss": 0.1893,
      "step": 11500
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 3.6123180389404297,
      "learning_rate": 0.00013813333333333335,
      "loss": 0.1717,
      "step": 11600
    },
    {
      "epoch": 1.56,
      "grad_norm": 2.9624767303466797,
      "learning_rate": 0.00013759999999999998,
      "loss": 0.1825,
      "step": 11700
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 2.008539915084839,
      "learning_rate": 0.00013706666666666667,
      "loss": 0.1759,
      "step": 11800
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 3.5187180042266846,
      "learning_rate": 0.00013653333333333334,
      "loss": 0.2061,
      "step": 11900
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.2248772382736206,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.1812,
      "step": 12000
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.23893515765666962,
      "learning_rate": 0.00013546666666666666,
      "loss": 0.1994,
      "step": 12100
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 1.601130723953247,
      "learning_rate": 0.00013493333333333332,
      "loss": 0.1629,
      "step": 12200
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 2.182973623275757,
      "learning_rate": 0.00013440000000000001,
      "loss": 0.1916,
      "step": 12300
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 2.535344362258911,
      "learning_rate": 0.00013386666666666668,
      "loss": 0.1566,
      "step": 12400
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 3.5265004634857178,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.1857,
      "step": 12500
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 4.889866352081299,
      "learning_rate": 0.0001328,
      "loss": 0.1759,
      "step": 12600
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.46739739179611206,
      "learning_rate": 0.00013226666666666667,
      "loss": 0.1822,
      "step": 12700
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 1.2541645765304565,
      "learning_rate": 0.00013173333333333333,
      "loss": 0.1777,
      "step": 12800
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.3160003125667572,
      "learning_rate": 0.00013120000000000002,
      "loss": 0.1773,
      "step": 12900
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 7.011572360992432,
      "learning_rate": 0.00013066666666666668,
      "loss": 0.2064,
      "step": 13000
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 6.4595818519592285,
      "learning_rate": 0.00013013333333333332,
      "loss": 0.1846,
      "step": 13100
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.278179407119751,
      "learning_rate": 0.0001296,
      "loss": 0.1803,
      "step": 13200
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 1.2278341054916382,
      "learning_rate": 0.00012906666666666667,
      "loss": 0.1735,
      "step": 13300
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 1.2789515256881714,
      "learning_rate": 0.00012853333333333336,
      "loss": 0.1938,
      "step": 13400
    },
    {
      "epoch": 1.8,
      "grad_norm": 3.4033281803131104,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.2009,
      "step": 13500
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 2.4996840953826904,
      "learning_rate": 0.00012746666666666666,
      "loss": 0.161,
      "step": 13600
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 2.908400774002075,
      "learning_rate": 0.00012693333333333335,
      "loss": 0.1681,
      "step": 13700
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.42412158846855164,
      "learning_rate": 0.0001264,
      "loss": 0.1756,
      "step": 13800
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 2.7589001655578613,
      "learning_rate": 0.00012586666666666667,
      "loss": 0.1777,
      "step": 13900
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.4536048471927643,
      "learning_rate": 0.00012533333333333334,
      "loss": 0.1815,
      "step": 14000
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.9268083572387695,
      "learning_rate": 0.0001248,
      "loss": 0.1969,
      "step": 14100
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 1.7484462261199951,
      "learning_rate": 0.00012426666666666666,
      "loss": 0.2068,
      "step": 14200
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.3103254437446594,
      "learning_rate": 0.00012373333333333335,
      "loss": 0.187,
      "step": 14300
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.5611092448234558,
      "learning_rate": 0.0001232,
      "loss": 0.1819,
      "step": 14400
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 15.0852632522583,
      "learning_rate": 0.00012266666666666668,
      "loss": 0.1982,
      "step": 14500
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 4.633594036102295,
      "learning_rate": 0.00012213333333333334,
      "loss": 0.1651,
      "step": 14600
    },
    {
      "epoch": 1.96,
      "grad_norm": 4.872343063354492,
      "learning_rate": 0.0001216,
      "loss": 0.1969,
      "step": 14700
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1.1462677717208862,
      "learning_rate": 0.00012106666666666666,
      "loss": 0.1871,
      "step": 14800
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.6344420313835144,
      "learning_rate": 0.00012053333333333334,
      "loss": 0.1786,
      "step": 14900
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.968787670135498,
      "learning_rate": 0.00012,
      "loss": 0.18,
      "step": 15000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9423684210526316,
      "eval_loss": 0.18779900670051575,
      "eval_runtime": 12.6584,
      "eval_samples_per_second": 600.392,
      "eval_steps_per_second": 9.401,
      "step": 15000
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.3232594132423401,
      "learning_rate": 0.00011946666666666668,
      "loss": 0.1479,
      "step": 15100
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 5.129280090332031,
      "learning_rate": 0.00011893333333333334,
      "loss": 0.1434,
      "step": 15200
    },
    {
      "epoch": 2.04,
      "grad_norm": 2.5869181156158447,
      "learning_rate": 0.0001184,
      "loss": 0.2023,
      "step": 15300
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 1.0615960359573364,
      "learning_rate": 0.00011786666666666668,
      "loss": 0.1635,
      "step": 15400
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 1.3396084308624268,
      "learning_rate": 0.00011733333333333334,
      "loss": 0.1625,
      "step": 15500
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.21081864833831787,
      "learning_rate": 0.00011679999999999999,
      "loss": 0.1652,
      "step": 15600
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 3.7334094047546387,
      "learning_rate": 0.00011626666666666668,
      "loss": 0.1632,
      "step": 15700
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 3.45740008354187,
      "learning_rate": 0.00011573333333333333,
      "loss": 0.1707,
      "step": 15800
    },
    {
      "epoch": 2.12,
      "grad_norm": 5.630903720855713,
      "learning_rate": 0.0001152,
      "loss": 0.1741,
      "step": 15900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 3.023595094680786,
      "learning_rate": 0.00011466666666666667,
      "loss": 0.1808,
      "step": 16000
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 1.775396704673767,
      "learning_rate": 0.00011413333333333333,
      "loss": 0.1679,
      "step": 16100
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.2466156482696533,
      "learning_rate": 0.0001136,
      "loss": 0.1564,
      "step": 16200
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 2.3578102588653564,
      "learning_rate": 0.00011306666666666667,
      "loss": 0.1534,
      "step": 16300
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 2.5083863735198975,
      "learning_rate": 0.00011253333333333334,
      "loss": 0.1713,
      "step": 16400
    },
    {
      "epoch": 2.2,
      "grad_norm": 4.47798490524292,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.1773,
      "step": 16500
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.4123081862926483,
      "learning_rate": 0.00011146666666666667,
      "loss": 0.1715,
      "step": 16600
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 2.6167991161346436,
      "learning_rate": 0.00011093333333333334,
      "loss": 0.1844,
      "step": 16700
    },
    {
      "epoch": 2.24,
      "grad_norm": 4.088836669921875,
      "learning_rate": 0.00011040000000000001,
      "loss": 0.1828,
      "step": 16800
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 3.402094602584839,
      "learning_rate": 0.00010986666666666668,
      "loss": 0.1708,
      "step": 16900
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.5257200002670288,
      "learning_rate": 0.00010933333333333333,
      "loss": 0.1606,
      "step": 17000
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.72391676902771,
      "learning_rate": 0.00010880000000000002,
      "loss": 0.1335,
      "step": 17100
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 2.866375684738159,
      "learning_rate": 0.00010826666666666668,
      "loss": 0.1966,
      "step": 17200
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.5803276896476746,
      "learning_rate": 0.00010773333333333333,
      "loss": 0.1465,
      "step": 17300
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.8271051049232483,
      "learning_rate": 0.00010720000000000002,
      "loss": 0.1605,
      "step": 17400
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 6.650928497314453,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.1652,
      "step": 17500
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.2078520655632019,
      "learning_rate": 0.00010613333333333333,
      "loss": 0.1533,
      "step": 17600
    },
    {
      "epoch": 2.36,
      "grad_norm": 3.9065186977386475,
      "learning_rate": 0.0001056,
      "loss": 0.1486,
      "step": 17700
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 4.808514595031738,
      "learning_rate": 0.00010506666666666667,
      "loss": 0.1983,
      "step": 17800
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 1.6444028615951538,
      "learning_rate": 0.00010453333333333333,
      "loss": 0.1569,
      "step": 17900
    },
    {
      "epoch": 2.4,
      "grad_norm": 3.0105855464935303,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.1868,
      "step": 18000
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 3.3548524379730225,
      "learning_rate": 0.00010346666666666667,
      "loss": 0.1598,
      "step": 18100
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 5.1322150230407715,
      "learning_rate": 0.00010293333333333335,
      "loss": 0.1711,
      "step": 18200
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.7159560918807983,
      "learning_rate": 0.00010240000000000001,
      "loss": 0.165,
      "step": 18300
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 5.569918155670166,
      "learning_rate": 0.00010186666666666667,
      "loss": 0.1564,
      "step": 18400
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 7.570582866668701,
      "learning_rate": 0.00010133333333333335,
      "loss": 0.1617,
      "step": 18500
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.6511130332946777,
      "learning_rate": 0.00010080000000000001,
      "loss": 0.1568,
      "step": 18600
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.9207841753959656,
      "learning_rate": 0.00010026666666666666,
      "loss": 0.1773,
      "step": 18700
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.2705610990524292,
      "learning_rate": 9.973333333333334e-05,
      "loss": 0.1778,
      "step": 18800
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.8625333905220032,
      "learning_rate": 9.92e-05,
      "loss": 0.1494,
      "step": 18900
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.8311493396759033,
      "learning_rate": 9.866666666666668e-05,
      "loss": 0.183,
      "step": 19000
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 2.256823778152466,
      "learning_rate": 9.813333333333334e-05,
      "loss": 0.1622,
      "step": 19100
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.36031630635261536,
      "learning_rate": 9.76e-05,
      "loss": 0.163,
      "step": 19200
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 2.3196730613708496,
      "learning_rate": 9.706666666666668e-05,
      "loss": 0.1369,
      "step": 19300
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 2.1049001216888428,
      "learning_rate": 9.653333333333334e-05,
      "loss": 0.1637,
      "step": 19400
    },
    {
      "epoch": 2.6,
      "grad_norm": 2.6749370098114014,
      "learning_rate": 9.6e-05,
      "loss": 0.1978,
      "step": 19500
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.18016771972179413,
      "learning_rate": 9.546666666666667e-05,
      "loss": 0.1597,
      "step": 19600
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.3526499271392822,
      "learning_rate": 9.493333333333334e-05,
      "loss": 0.1723,
      "step": 19700
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.0785715579986572,
      "learning_rate": 9.44e-05,
      "loss": 0.1709,
      "step": 19800
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 7.365818977355957,
      "learning_rate": 9.386666666666667e-05,
      "loss": 0.1556,
      "step": 19900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 9.093605041503906,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.1585,
      "step": 20000
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.5158190727233887,
      "learning_rate": 9.28e-05,
      "loss": 0.1823,
      "step": 20100
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 3.7852485179901123,
      "learning_rate": 9.226666666666667e-05,
      "loss": 0.1977,
      "step": 20200
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.20557333528995514,
      "learning_rate": 9.173333333333333e-05,
      "loss": 0.163,
      "step": 20300
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 14.585860252380371,
      "learning_rate": 9.120000000000001e-05,
      "loss": 0.1703,
      "step": 20400
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 2.8018877506256104,
      "learning_rate": 9.066666666666667e-05,
      "loss": 0.182,
      "step": 20500
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 1.406253695487976,
      "learning_rate": 9.013333333333333e-05,
      "loss": 0.1333,
      "step": 20600
    },
    {
      "epoch": 2.76,
      "grad_norm": 4.321880340576172,
      "learning_rate": 8.960000000000001e-05,
      "loss": 0.1861,
      "step": 20700
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 3.6048762798309326,
      "learning_rate": 8.906666666666667e-05,
      "loss": 0.1727,
      "step": 20800
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 2.1031949520111084,
      "learning_rate": 8.853333333333333e-05,
      "loss": 0.1284,
      "step": 20900
    },
    {
      "epoch": 2.8,
      "grad_norm": 4.651261329650879,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.1874,
      "step": 21000
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 5.958615779876709,
      "learning_rate": 8.746666666666667e-05,
      "loss": 0.1707,
      "step": 21100
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 9.765973091125488,
      "learning_rate": 8.693333333333334e-05,
      "loss": 0.1744,
      "step": 21200
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.6600468158721924,
      "learning_rate": 8.64e-05,
      "loss": 0.1345,
      "step": 21300
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 2.452624797821045,
      "learning_rate": 8.586666666666668e-05,
      "loss": 0.185,
      "step": 21400
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 3.7778337001800537,
      "learning_rate": 8.533333333333334e-05,
      "loss": 0.1704,
      "step": 21500
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.4872805178165436,
      "learning_rate": 8.48e-05,
      "loss": 0.1761,
      "step": 21600
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 1.457427978515625,
      "learning_rate": 8.426666666666668e-05,
      "loss": 0.1809,
      "step": 21700
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 7.154353141784668,
      "learning_rate": 8.373333333333334e-05,
      "loss": 0.1542,
      "step": 21800
    },
    {
      "epoch": 2.92,
      "grad_norm": 2.7854158878326416,
      "learning_rate": 8.32e-05,
      "loss": 0.1757,
      "step": 21900
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 3.039529323577881,
      "learning_rate": 8.266666666666667e-05,
      "loss": 0.1789,
      "step": 22000
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 3.4557976722717285,
      "learning_rate": 8.213333333333334e-05,
      "loss": 0.14,
      "step": 22100
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.722401738166809,
      "learning_rate": 8.16e-05,
      "loss": 0.1465,
      "step": 22200
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 4.401748180389404,
      "learning_rate": 8.106666666666667e-05,
      "loss": 0.135,
      "step": 22300
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 4.3515167236328125,
      "learning_rate": 8.053333333333334e-05,
      "loss": 0.2009,
      "step": 22400
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.5809648633003235,
      "learning_rate": 8e-05,
      "loss": 0.1659,
      "step": 22500
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9453947368421053,
      "eval_loss": 0.17843714356422424,
      "eval_runtime": 11.9917,
      "eval_samples_per_second": 633.772,
      "eval_steps_per_second": 9.924,
      "step": 22500
    }
  ],
  "logging_steps": 100,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.392609480704e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
